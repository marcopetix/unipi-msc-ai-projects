{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# General Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar \n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#our code \n",
    "from USPPM_model import USPPPM_model\n",
    "from USPPM_dataset import set_max_len\n",
    "from USPPM_datamodule import USPPPM_datamodule\n",
    "\n",
    "from pynvml import *\n",
    "import argparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from pytorch_lightning.callbacks.callback import Callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = {\n",
    "        \"distilbert-base-uncased\":32,\n",
    "        \"bert-base-uncased\":64,\n",
    "        \"Yanhao/simcse-bert-for-patent\":64,\n",
    "        \"ahotrod/electra_large_discriminator_squad2_512\":32,\n",
    "        \"microsoft/deberta-v3-large\":8\n",
    "        }\n",
    "\n",
    "features = [\n",
    "           'CPCdescription_same_anchor_context_similar_targets',\n",
    "           'anchor_target_CPCdescription',\n",
    "            # do not add this #'same_anchor_similar_targets',\n",
    "           'same_anchor_context_targets',\n",
    "           'same_anchor_context_similar_targets'\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: distilbert-base-uncased\n",
      "GPU: 1\n",
      "Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "model_name = list(batch_sizes.keys())[0]\n",
    "batch_size = batch_sizes[model_name]\n",
    "\n",
    "out_dir_prefix = \"final_train_\"\n",
    "gpu_id = \"1\"\n",
    "\n",
    "print(\"Model: \" + model_name)\n",
    "print(\"GPU: \" + gpu_id)\n",
    "print(\"Batch size: \" + str(batch_size))\n",
    "\n",
    "\n",
    "# Defining a search space!\n",
    "config_dict = {\n",
    "    \"debug_samples\": 1500,\n",
    "    \"DEBUG\": True,\n",
    "    \"target_size\" : 1,\n",
    "    \"num_workers\" : 8,\n",
    "    # Training parameters\n",
    "    \"batch_size\" : batch_size,\n",
    "    \"epochs\" : 8,\n",
    "    \"warmup_steps\" : 0,\n",
    "    \"min_lr\" : 1e-6,\n",
    "    \"encoder_lr\" : 2e-5,\n",
    "    \"decoder_lr\" : 2e-5,\n",
    "    \"eps\" : 1e-6,\n",
    "    \"betas\" : (0.9, 0.999),\n",
    "    \"weight_decay\" : 0.01,\n",
    "    \"fc_dropout\" : 0.2,\n",
    "    \"seed\" : 42,\n",
    "    \"train_test_split\": 1,\n",
    "    \"loss\": \"pearson\",\n",
    "    \"stratify_on\" : 'stratification_index',\n",
    "    \"features\" : features[2],\n",
    "    \"model\" : model_name,\n",
    "    }\n",
    "\n",
    "INPUT_DIR = '../dataset/us-patent-phrase-to-phrase-matching/'\n",
    "\n",
    "visible_devices = gpu_id\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=visible_devices\n",
    "num_gpus = len(visible_devices.split(\",\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./train_dataframe_with_features.csv\")\n",
    "test_df = pd.read_csv(\"./test_dataframe_with_features.csv\")\n",
    "if config_dict[\"DEBUG\"]:\n",
    "    train_df = train_df.iloc[:config_dict[\"debug_samples\"],:]\n",
    "\n",
    "metrics = {\"train_loss\" : \"train_loss\", \"val_loss\":\"val_loss\", \"val_score\":\"val_score\",\"train_score\":\"train_score\", \"batch_size\":\"batch_size\",\"fold\":\"fold\", \"epoch\":\"epoch\"}\n",
    "\n",
    "#trial_id = ray.air.session.get_trial_id()\n",
    "OUTPUT_DIR = './'\n",
    "logging_dir = f\"USPPPM\"\n",
    "\n",
    "for d in [OUTPUT_DIR, \"lightning_logs/\"+logging_dir]:\n",
    "    try:\n",
    "        os.makedirs(d)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=logging_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37228e313dfa4470aa88a22786f2575e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name       | Type            | Params\n",
      "-----------------------------------------------\n",
      "0 | model      | DistilBertModel | 66.4 M\n",
      "1 | fc_dropout | Dropout         | 0     \n",
      "2 | fc         | Linear          | 769   \n",
      "3 | attention  | Sequential      | 394 K \n",
      "-----------------------------------------------\n",
      "66.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "66.8 M    Total params\n",
      "267.032   Total estimated model params size (MB)\n",
      "\n",
      "  | Name       | Type            | Params\n",
      "-----------------------------------------------\n",
      "0 | model      | DistilBertModel | 66.4 M\n",
      "1 | fc_dropout | Dropout         | 0     \n",
      "2 | fc         | Linear          | 769   \n",
      "3 | attention  | Sequential      | 394 K \n",
      "-----------------------------------------------\n",
      "66.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "66.8 M    Total params\n",
      "267.032   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ea974a9c6b4007afc85a129587fedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:231: UserWarning: You called `self.log('batch_size', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/storagenfs/m.petix/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:231: UserWarning: You called `self.log('epoch', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "Epoch -1, global step 43: 'train_loss' reached 0.52956 (best 0.52956), saving model to '/storagenfs/m.petix/hlt_usppm/src/checkpoints/best_checkpoint.ckpt' as top 1\n",
      "Epoch -1, global step 43: 'train_loss' reached 0.52956 (best 0.52956), saving model to '/storagenfs/m.petix/hlt_usppm/src/checkpoints/best_checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs: 0\n",
      "Number of Infs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 86: 'train_loss' reached 0.38812 (best 0.38812), saving model to '/storagenfs/m.petix/hlt_usppm/src/checkpoints/best_checkpoint.ckpt' as top 1\n",
      "Epoch 0, global step 86: 'train_loss' reached 0.38812 (best 0.38812), saving model to '/storagenfs/m.petix/hlt_usppm/src/checkpoints/best_checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs: 0\n",
      "Number of Infs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 129: 'train_loss' reached 0.19204 (best 0.19204), saving model to '/storagenfs/m.petix/hlt_usppm/src/checkpoints/best_checkpoint.ckpt' as top 1\n",
      "Epoch 1, global step 129: 'train_loss' reached 0.19204 (best 0.19204), saving model to '/storagenfs/m.petix/hlt_usppm/src/checkpoints/best_checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs: 0\n",
      "Number of Infs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 172: 'train_loss' was not in top 1\n",
      "Epoch 2, global step 172: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs: 0\n",
      "Number of Infs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 215: 'train_loss' reached 0.14754 (best 0.14754), saving model to '/storagenfs/m.petix/hlt_usppm/src/checkpoints/best_checkpoint.ckpt' as top 1\n",
      "Epoch 3, global step 215: 'train_loss' reached 0.14754 (best 0.14754), saving model to '/storagenfs/m.petix/hlt_usppm/src/checkpoints/best_checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs: 0\n",
      "Number of Infs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 258: 'train_loss' was not in top 1\n",
      "Epoch 4, global step 258: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs: 0\n",
      "Number of Infs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 301: 'train_loss' reached 0.10894 (best 0.10894), saving model to '/storagenfs/m.petix/hlt_usppm/src/checkpoints/best_checkpoint.ckpt' as top 1\n",
      "Epoch 5, global step 301: 'train_loss' reached 0.10894 (best 0.10894), saving model to '/storagenfs/m.petix/hlt_usppm/src/checkpoints/best_checkpoint.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs: 0\n",
      "Number of Infs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 344: 'train_loss' was not in top 1\n",
      "Epoch 6, global step 344: 'train_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=8` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs: 0\n",
      "Number of Infs: 0\n"
     ]
    }
   ],
   "source": [
    "done_pre_evaluation = False\n",
    "\n",
    "# this try catch is needed to properly terminate the run\n",
    "\n",
    "pl.seed_everything(config_dict[\"seed\"])\n",
    "\n",
    "steps_per_epoch = len(train_df) * config_dict['train_test_split'] // config_dict['batch_size']\n",
    "config_dict['training_steps'] = steps_per_epoch * config_dict['epochs']\n",
    "config_dict['warmup_steps'] = int(config_dict['training_steps'] * config_dict['warmup_steps'])\n",
    "                            \n",
    "set_max_len(config_dict, train_df)  \n",
    "\n",
    "callbacks = [\n",
    "            #TuneReportCallback(metrics, on=\"epoch_end\"),\n",
    "            ModelCheckpoint(\n",
    "                dirpath=f\"checkpoints/\",\n",
    "                filename=\"best_checkpoint\",\n",
    "                save_top_k=1,\n",
    "                verbose=True,\n",
    "                monitor='train_loss',\n",
    "                mode='min'\n",
    "            ), \n",
    "            EarlyStopping(monitor='train_score', patience=2,mode='max'), \n",
    "            TQDMProgressBar(refresh_rate=100)\n",
    "            ]\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "datamodule = USPPPM_datamodule(config_dict, 0.9, train_df, test_df)\n",
    "model = USPPPM_model(config_dict)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        num_sanity_val_steps=0,\n",
    "        check_val_every_n_epoch=1,\n",
    "        callbacks=callbacks,\n",
    "        max_epochs=config_dict['epochs'],\n",
    "        min_epochs=2,\n",
    "        devices=[1], # lightning sees only the gpu that is being assigned to this instance of trainable, so it will be always 0 even if it's using gpu 1,2 or 3\n",
    "        accelerator=\"gpu\",\n",
    "        limit_val_batches = 0.0 # needed to skip validation\n",
    "        )\n",
    "\n",
    "datamodule.setup()\n",
    "model.epoch=-1\n",
    "trainer.validate(model, datamodule)\n",
    "done_pre_evaluation = True\n",
    " \n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-898f5340fc67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUSPPPM_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~/ray_results/final_train_distilbert-base-uncased/trainable_e42a4_00000_0_2023-04-16_19-48-27/checkpoints/best_checkpoint.ckpt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         return _load_from_checkpoint(\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmap_location\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mpl_legacy_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/migration.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;31m# `pl.utilities.argparse_utils` was renamed to `pl.utilities.argparse`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mlegacy_argparse_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModuleType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pytorch_lightning.utilities.argparse_utils\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = USPPPM_model.load_from_checkpoint(\"~/ray_results/final_train_distilbert-base-uncased/trainable_e42a4_00000_0_2023-04-16_19-48-27/checkpoints/best_checkpoint.ckpt\", config_dict=config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(model, datamodule.train_dataloader(), return_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748b30cb0a9a472ab2264203269ca464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 43it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'tuple'> <class 'int'>\n",
      "[[0.4135774 ]\n",
      " [0.5905266 ]\n",
      " [0.33691028]\n",
      " [0.4095657 ]\n",
      " [0.35680413]\n",
      " [0.39533228]\n",
      " [0.4374867 ]\n",
      " [0.26101255]\n",
      " [0.5007539 ]\n",
      " [0.6555991 ]\n",
      " [0.49835995]\n",
      " [0.5257712 ]\n",
      " [0.41701564]\n",
      " [0.48323244]\n",
      " [0.42927295]\n",
      " [0.47450602]\n",
      " [0.31133038]\n",
      " [0.39408654]\n",
      " [0.62861735]\n",
      " [0.52045375]\n",
      " [0.4139055 ]\n",
      " [0.3575145 ]\n",
      " [0.36369085]\n",
      " [0.3474173 ]\n",
      " [0.5636541 ]\n",
      " [0.29699004]\n",
      " [0.24356504]\n",
      " [0.38855627]\n",
      " [0.39017656]\n",
      " [0.44193944]\n",
      " [0.3332249 ]\n",
      " [0.21477272]]\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(model, datamodule.test_dataloader(), return_predictions=True)\n",
    "print(type(predictions), type(predictions[0]), type(predictions[0][0]))\n",
    "print(predictions[0][1].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4136],\n",
       "        [0.5905],\n",
       "        [0.3369],\n",
       "        [0.4096],\n",
       "        [0.3568],\n",
       "        [0.3953],\n",
       "        [0.4375],\n",
       "        [0.2610],\n",
       "        [0.5008],\n",
       "        [0.6556],\n",
       "        [0.4984],\n",
       "        [0.5258],\n",
       "        [0.4170],\n",
       "        [0.4832],\n",
       "        [0.4293],\n",
       "        [0.4745],\n",
       "        [0.3113],\n",
       "        [0.3941],\n",
       "        [0.6286],\n",
       "        [0.5205],\n",
       "        [0.4139],\n",
       "        [0.3575],\n",
       "        [0.3637],\n",
       "        [0.3474],\n",
       "        [0.5637],\n",
       "        [0.2970],\n",
       "        [0.2436],\n",
       "        [0.3886],\n",
       "        [0.3902],\n",
       "        [0.4419],\n",
       "        [0.3332],\n",
       "        [0.2148]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['score'] = predictions[0][1].numpy()\n",
    "test_df[['id','score']].to_csv(\"test_predictions.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
